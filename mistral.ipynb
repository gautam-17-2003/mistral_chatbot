{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import json\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_db={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(message, history):\n",
    "    prompt = \"<s>\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "        prompt += f\" {bot_response}</s> \"\n",
    "        my_db[user_prompt]=bot_response\n",
    "    prompt += f\"[INST] {message} [/INST]\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt, history, temperature=0.9, max_new_tokens=256, top_p=0.95, repetition_penalty=1.0,\n",
    "):\n",
    "    temperature = float(temperature)\n",
    "    if temperature < 1e-2:\n",
    "        temperature = 1e-2\n",
    "    top_p = float(top_p)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        seed=42,\n",
    "    )\n",
    "    print(\"prompt\",prompt)\n",
    "    print('history',history)\n",
    "    formatted_prompt = format_prompt(prompt, history)\n",
    "\n",
    "    stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=True, details=True, return_full_text=False)\n",
    "    output = \"\"\n",
    "\n",
    "    for response in stream:\n",
    "        output += response.token.text\n",
    "        yield output\n",
    "    my_db[prompt]=output\n",
    "    # print(my_db)\n",
    "    os.chdir('./chat data')\n",
    "    _file_name=\"\"\n",
    "    for  i in time.ctime().split(\" \"):\n",
    "        _file_name += i\n",
    "    file_name =\"\"\n",
    "    for i,name in enumerate(_file_name.split(\":\")):\n",
    "        if i<=0:\n",
    "            file_name+=name\n",
    "        else:\n",
    "            file_name+='_'+name\n",
    "    # print(file_name)\n",
    "\n",
    "    json_data = json.dumps(my_db, indent=4)  # `indent` for pretty formatting (optional)\n",
    "\n",
    "    with open(f\"{file_name}.json\", \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "    os.chdir('C:\\gautam\\gpt cli')\n",
    "    print(\"output\",output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hi'\n",
    "history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate(prompt, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function generator.throw>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.throw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
